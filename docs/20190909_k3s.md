
# An IoT Solution by K3S+ GitOps on Raspberry-Pi

<!-- TOC depthFrom:2 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [Solution Objective](#solution-objective)
- [Features](#features)
- [Architecture](#architecture)
		- [Overview](#overview)
		- [Infrastructure](#infrastructure)
		- [Workflow](#workflow)
		- [Network](#network)
		- [Security](#security)
		- [High-Availability](#high-availability)
		- [Monitoring](#monitoring)
- [Installation and Configuration](#installation-and-configuration)
		- [On ALL Nodes, if within GFW](#on-all-nodes-if-within-gfw)
		- [Master](#master)
		- [Worker](#worker)
		- [Add New Worker](#add-new-worker)
		- [Bash Auto Completion](#bash-auto-completion)
		- [Install ```helm``` (tiller)](#install-helm-tiller)
		- [Check Tiller's status and find the following error due to GFW](#check-tillers-status-and-find-the-following-error-due-to-gfw)
		- [Install ```fluxcd```](#install-fluxcd)
		- [Install and Configure ```fluxctl```](#install-and-configure-fluxctl)
- [Why K3S](#why-k3s)

<!-- /TOC -->

## Solution Highlights

- IoT, lightweight and isolate application from infrastructure
- You declaratively describe the entire desired state of your system in git
- Automate everything and you manage code rather than container
- Less maintenance with high tolerance
- Natural CI/ CD

## Architecture

#### Infrastructure

- K3S + GitOps
- Raspberry-Pi software stack

<center><img src="../imgs/20190909_k3s_overview.png"></center>

#### Workflow

- GitOps' fluxcd = automated git -> cluster synchronisation

<center><img src="../imgs/20190909_flux-cd-diagram.png" width="700px"></center>

> Reference > https://docs.fluxcd.io/en/latest/introduction.html
> Image credit > https://github.com/fluxcd/flux

#### Network and Security

<center><img src="../imgs/20190909_k3s_net.png"></center>

- SSL, managed by k3s
- Software-firewall being used

#### High-Availability

#### Monitoring

## Installation and Configuration

#### On ALL Nodes, if within GFW

> Reference > https://rancher.com/docs/k3s/latest/en/running/

- A pre-requisite step on __ALL__ nodes, if within GFW or air-gap env, before install starts and if adding new node, this is a must as well

  ```bash

  # Download the appropriate release
  wget https://github.com/rancher/k3s/releases/download/v0.9.0/k3s-airgap-images-amd64.tar

  sudo mkdir -p /var/lib/rancher/k3s/agent/images/
  sudo cp pause-3.1.tar               /var/lib/rancher/k3s/agent/images/
  sudo cp k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/

  ```

#### Master

- Make ```kubeconfig``` readable to non-root user,

  ```bash
  curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
  ```

  or
    ```bash
    curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -s -
    ```

  or if want to use ```docker``` instead of ```containerd```

    ```bash
    curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--docker" sh -
    ```

#### Worker

On master node

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
K104dcf919a073e2335312f1bee5e41a024d76c19d86d91190661069cd8df80ee6f::node:7353837f5b3029369df7c7a7ad27d280
```

On worker node(s)
```bash
export K3S_URL="https://192.168.100.31:6443"
export K3S_TOKEN="K104dcf919a073e2335312f1bee5e41a024d76c19d86d91190661069cd8df80ee6f::node:7353837f5b3029369df7c7a7ad27d280"

curl -sfL https://get.k3s.io | K3S_URL=${K3S_URL} K3S_TOKEN=${K3S_TOKEN} sh -
```

Check on master
```bash
kubectl get nodes
```

Check journal
```bash
journalctl -u k3s-agent
```

#### Add New Worker

```bash
# Download the appropriate release. You need arm64 binary for Raspberry-Pi device
wget https://github.com/rancher/k3s/releases/download/v0.9.0/k3s-airgap-images-amd64.tar

sudo mkdir -p /var/lib/rancher/k3s/agent/images/
sudo cp pause-3.1.tar               /var/lib/rancher/k3s/agent/images/
sudo cp k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/
```

#### Bash Auto Completion

- Setup env

  ```bash
  source <(kubectl completion bash)
  echo "source <(kubectl completion bash)" >> ~/.bashrc

  alias k=kubectl
  complete -F __start_kubectl k
  ```

#### Install ```helm``` (```tiller```)

```bash
cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
```

```bash
$ curl -LO https://git.io/get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh
```

```bash
# Create a service account for tiller
kubectl -n kube-system create sa tiller

# Create a cluster role binding for tiller
kubectl create clusterrolebinding tiller-cluster-rule \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:tiller

# Deploy tiller to kube-system
helm init --skip-refresh --upgrade --service-account tiller --history-max 20
```

#### Check ```tiller``` status and find the following error due to GFW

```bash
kubectl -n kube-system describe pod tiller-deploy-75f6c87b87-4f8zc

Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57s                default-scheduler  Successfully assigned kube-system/tiller-deploy-75f6c87b87-4f8zc to node05
  Warning  Failed     26s                kubelet, node05    Failed to pull image "gcr.io/kubernetes-helm/tiller:v2.14.3": rpc error: code = Unknown desc = failed to resolve image "gcr.io/kubernetes-helm/tiller:v2.14.3": no available registry endpoint: failed to do request: Head https://gcr.io/v2/kubernetes-helm/tiller/manifests/v2.14.3: dial tcp 74.125.23.82:443: i/o timeout
  Warning  Failed     26s                kubelet, node05    Error: ErrImagePull
  Normal   BackOff    25s                kubelet, node05    Back-off pulling image "gcr.io/kubernetes-helm/tiller:v2.14.3"
  Warning  Failed     25s                kubelet, node05    Error: ImagePullBackOff
  Normal   Pulling    13s (x2 over 56s)  kubelet, node05    Pulling image "gcr.io/kubernetes-helm/tiller:v2.14.3"
```

The __workaround__ is to place ```tiller.tar``` into _specific_ k3s' images path ```/var/lib/rancher/k3s/agent/images/``` on __each__ agent/ worker
```bash
docker pull sapcc/tiller:v2.14.3
docker tag sapcc/tiller:v2.14.3 gcr.io/kubernetes-helm/tiller:v2.14.3
docker save gcr.io/kubernetes-helm/tiller:v2.14.3 > ./tiller.tar
sudo cp tiller.tar /var/lib/rancher/k3s/agent/images/
sudo systemctl restart k3s-agent.service
```

#### Install ```fluxcd```

> Reference > https://docs.fluxcd.io/en/latest/tutorials/get-started-helm.html

```bash
kubectl -n kube-system create sa tiller
kubectl create clusterrolebinding tiller-cluster-rule \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-system:tiller

# Deploy tiller in cluster
helm init --skip-refresh --upgrade --service-account tiller --history-max 10

# Add repo
helm repo add fluxcd https://charts.fluxcd.io

# Apply the Helm Release CRD
kubectl apply -f https://raw.githubusercontent.com/fluxcd/flux/helm-0.10.1/deploy-helm/flux-helm-release-crd.yaml
```

Fork ```fluxcd/flux-get-started``` on GitHub and replace the ```fluxcd``` with your GitHub username

> Reference > https://docs.fluxcd.io/en/latest/tutorials/get-started-helm.html#install-flux

```bash
helm upgrade -i flux \
  --set helmOperator.create=true \
  --set helmOperator.createCRD=false \
  --set git.url=git@github.com:j3ffyang/flux-get-started \
  --namespace flux fluxcd/flux
```

You would have

```bash
kubectl -n flux get all
NAME                                      READY   STATUS    RESTARTS   AGE
pod/flux-memcached-b8f88949-zsnhm         1/1     Running   0          4m47s
pod/flux-helm-operator-6bcb497545-dkpcm   1/1     Running   0          4m47s
pod/flux-7c4c75f86-kt68f                  1/1     Running   0          4m47s


NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
service/flux-memcached   ClusterIP   10.43.240.190   <none>        11211/TCP   4m47s
service/flux             ClusterIP   10.43.136.163   <none>        3030/TCP    4m47s


NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/flux-memcached       1/1     1            1           4m47s
deployment.apps/flux-helm-operator   1/1     1            1           4m47s
deployment.apps/flux                 1/1     1            1           4m47s

NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/flux-memcached-b8f88949         1         1         1       4m47s
replicaset.apps/flux-helm-operator-6bcb497545   1         1         1       4m47s
replicaset.apps/flux-7c4c75f86                  1         1         1       4m47s
```

#### Install and Configure ```fluxctl```

```bash
https://github.com/fluxcd/flux/releases/download/1.14.2/fluxctl_linux_amd64

sudo cp fluxctl_linux_amd64 /usr/local/bin/fluxctl
sudo chmod 755 /usr/local/bin/fluxctl
fluxctl --k8s-fwd-ns=flux list-workloads
```

Get the access key

```bash
fluxctl identity --k8s-fwd-ns flux
```

In order to sync your cluster state with git you need to copy the public key and create a deploy key with write access on your GitHub repository.

Open GitHub, navigate to your fork, go to ```Settings``` > ```Deploy keys```, click on ```Add deploy key```, give it a ```Title```, check ```Allow write access```, paste the Flux public key and click ```Add key```.

(Or replace YOURUSER with your GitHub ID in this url: https://github.com/YOURUSER/flux-get-started/settings/keys/new and paste the key there.)

```bash
helm list --namespace demo

NAME   	REVISION	UPDATED                 	STATUS  	CHART        	APP VERSION	NAMESPACE
mongodb	1       	Wed Sep 25 18:31:56 2019	DEPLOYED	mongodb-4.9.0	4.0.3      	demo     
```

#### Commit a Change

```bash
git clone git@github.com:j3ffyang/flux-get-started.git
```

```bash
cd flux-get-started
vi releases/mongodb.yaml
```

```yaml
  values:
    image:
      repository: bitnami/mongodb
      tag: 4.0.6
```

Then commit
```bash
git add --all; git commit --message "upgrade mongo tag from 403 to 406"; git push
```

Check the Change
```bash
kubectl -n flux logs deployment/flux -f
kubectl describe -n demo deployment/mongodb | grep Image
kubectl -n demo get all
```

You would see
```bash
kubectl describe -n demo deployment/mongodb | grep Image
    Image:      docker.io/bitnami/mongodb:4.0.6
```

## Why K3S

- Kubernetes orchestration
- Managing compute resource, network, volume and security
- Built-in high-availability, fault-tolerance and load-balancing, sort of self-healing feature
- Rich plugins like dark-launching/ A/B-testing
- Natural integration with CI/ CD
- Workload manageability
